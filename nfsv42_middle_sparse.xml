<section anchor="sec:sparse" title="Simple and Efficient Read Support for Sparse Files">
  <section anchor="ss:sparse:intro" title="Introduction">
    <t>
      NFS is now used in many data centers as the sole or primary method of
      data access.  Consequently, more types of applications are using NFS
      than ever before, each with their own requirements and generated
      workloads.  As part of this, sparse files are increasing in number
      while NFS continues to lack any specific knowledge of a sparse file's
      layout.  This document puts forth a proposal for the NFSv4.2 protocol
      to support efficient reading of sparse files.
    </t>

    <t>
      A sparse file is a common way of representing a large file without
      having to reserve disk space for it.  Consequently, a sparse file
      uses less physical space than its size indicates.  This means the
      file contains 'holes', byte ranges within the file that contain no
      data.  Most modern file systems support sparse files, including most
      UNIX file systems and NTFS, but notably not Apple's HFS+.  Common
      examples of sparse files include VM OS/disk images, database files,
      log files, and even checkpoint recovery files most commonly used by
      the HPC community.
    </t>

    <t>
      If an application reads a hole in a sparse file, the file system must
      returns all zeros to the application.   For local data access there
      is little penalty, but with NFS these zeroes must be transferred back
      to the client.  If an application uses the NFS client to read data
      into memory, this wastes time and bandwidth as the application waits
      for the zeroes to be transferred.  Once the zeroes arrive, they then
      steal memory or cache space from real data.  To make matters worse,
      if an application then proceeds to write data to another file system,
      the zeros are written into the file, expanding the sparse file into a
      full sized regular file.  Beyond wasting disk space, this can
      actually prevent large sparse files from ever being copied to another
      storage location due to space limitations.
    </t>

    <t>
      This document adds a new READPLUS operation to efficiently read from
      sparse files by avoiding the transfer of all zero regions from the
      server to the client.  READPLUS supports all the features of READ but
      includes a minimal extension to support sparse files.  In addition,
      the return value of READPLUS is now compatible with NFSv4.1 minor
      versioning rules and could support other future extensions without
      requiring yet another operation.  READPLUS is guaranteed to perform
      no worse than READ, and can dramatically improve performance with
      sparse files.  READPLUS does not depend on pNFS protocol features,
      but can be used by pNFS to support sparse files.
    </t>
  </section>

  <section anchor="ss:sparse:terms" title="Terminology">

    <t>
      <list style="hanging">
        <t hangText="Regular file">
          Regular file: An object of file type NF4REG or NF4NAMEDATTR.
        </t>

        <t hangText="Sparse file">
          Sparse File. A Regular file that contains one or more Holes.
        </t>

        <t hangText="Hole">
          Hole. A byte range within a Sparse file that contains regions of
          all zeroes.  For block-based file systems, this could also be an
          unallocated region of the file.
        </t>
      </list>
    </t>
  </section>

  <section anchor="ss:sparse:apps" title="Applications and Sparse Files">
    <t>
      Applications may cause an NFS client to read holes in a file for
      several reasons.  This section describes three different application
      workloads that cause the NFS client to transfer data unnecessarily.
      These workloads are simply examples, and there are probably many more
      workloads that are negatively impacted by sparse files.
    </t>

    <t>
      The first workload that can cause holes to be read is sequential
      reads within a sparse file.  When this happens, the NFS client may
      perform read requests ("readahead") into sections of the file not
      explicitly requested by the application.  Since the NFS client cannot
      differentiate between holes and non-holes, the NFS client may
      prefetch empty sections of the file.
    </t>

    <t>
      This workload is exemplified by Virtual Machines and their associated
      file system images, e.g., VMware .vmdk files, which are large sparse
      files encapsulating an entire operating system.  If a VM reads files
      within the file system image, this will translate to sequential NFS
      read requests into the much larger file system image file.  Since NFS
      does not understand the internals of the file system image, it ends
      up performing readahead file holes.
    </t>

    <t>
      The second workload is generated by copying a file from a directory
      in NFS to either the same NFS server, to another file system, e.g.,
      another NFS or Samba server, to a local ext3 file system, or even a
      network socket.  In this case, bandwidth and server resources are
      wasted as the entire file is transferred from the NFS server to the
      NFS client.   Once a byte range of the file has been transferred to
      the client, it is up to the client application, e.g., rsync, cp, scp,
      on how it writes the data to the target location.  For example, cp
      supports sparse files and will not write all zero regions, whereas
      scp does not support sparse files and will transfer every byte of the
      file.
    </t>

    <t>
      The third workload is generated by applications that do not utilize
      the NFS client cache, but instead use direct I/O and manage cached
      data independently, e.g., databases.  These applications may perform
      whole file caching with sparse files, which would mean that even the
      holes will be transferred to the clients and cached.
    </t>
  </section>

  <section anchor="ss:sparse:overiew" title="Overview of Sparse Files and NFSv4">
    <t>
      This proposal seeks to provide sparse file support to the largest
      number of NFS client and server implementations, and as such proposes
      to add a new return code to the mandatory NFSv4.1 READPLUS operation
      instead of proposing additions or extensions of new or existing
      optional features (such as pNFS).
    </t>

    <t>
      As well, this document seeks to ensure that the proposed extensions
      are simple and do not transfer data between the client and server
      unnecessarily. For example, one possible way to implement sparse file
      read support would be to have the client, on the first hole
      encountered or at OPEN time, request a Data Region Map from the
      server.  A Data Region Map would specify all zero and non-zero
      regions in a file.  While this option seems simple, it is less useful
      and can become inefficient and cumbersome for several reasons:
    </t>

    <t>
      <list style="symbols">
        <t>
          Data Region Maps can be large, and transferring them can reduce
          overall read performance.  For example, VMware's .vmdk files can
          have a file size of over 100 GBs and have a map well over several
          MBs.
        </t>

        <t>
          Data Region Maps can change frequently, and become invalidated on
          every write to the file.  This can result the map being
          transferred multiple times with each update to the file.  For
          example, a VM that updates a config file in its file system image
          would invalidate the Data Region Map not only for itself, but for
          all other clients accessing the same file system image.
        </t>

        <t>
          Data Region Maps do not handle all zero-filled sections of the
          file, reducing the effectiveness of the solution. While it may be
          possible to modify the maps to handle zero-filled sections (at
          possibly great effort to the server), it is almost impossible with
          pNFS.  With pNFS, the owner of the Data Region Map is the metadata
          server, which is not in the data path and has no knowledge of the
          contents of a data region.
        </t>
      </list>
    </t>

    <t>
      Another way to handle holes is compression, but this not ideal since
      it requires all implementations to agree on a single compression
      algorithm and requires a fair amount of computational overhead.
    </t>

    <t>
      Note that supporting writing to a sparse file does not require
      changes to the protocol.  Applications and/or NFS implementations can
      choose to ignore WRITE requests of all zeroes to the NFS server
      without consequence.
    </t>
  </section>

  <section anchor="ss:sparse:readplus" title="Operation 65: READPLUS">
    <t>
      The section introduces a new read operation, named READPLUS, which
      allows NFS clients to avoid reading holes in a sparse file. READPLUS
      is guaranteed to perform no worse than READ, and can dramatically
      improve performance with sparse files.
    </t>

    <t>
      READPLUS supports all the features of the existing NFSv4.1 READ
      operation <xref target="RFC5661" /> and adds a simple yet
      significant extension to the
      format of its response.  The change allows the client to avoid
      returning all zeroes from a file hole, wasting computational and
      network resources and reducing performance.  READPLUS uses a new
      result structure that tells the client that the result is all zeroes
      AND the byte-range of the hole in which the request was made.
      Returning the hole's byte-range, and only upon request, avoids
      transferring large Data Region Maps that may be soon invalidated and
      contain information about a file that may not even be read in its
      entirely.
    </t>

    <t>
      A new read operation is required due to NFSv4.1 minor versioning
      rules that do not allow modification of existing operation's
      arguments or results.  READPLUS is designed in such a way to allow
      future extensions to the result structure.  The same approach could
      be taken to extend the argument structure, but a good use case is
      first required to make such a change.
    </t>

    <section title="ARGUMENT">
      <?rfc include='autogen/copy_notify_args.xml'?>
    </section>

    <section title="RESULT">
      <?rfc include='autogen/copy_notify_res.xml'?>
    </section>

    <section title="DESCRIPTION">

      <t>
        The READPLUS operation is based upon the NFSv4.1 READ operation
        <xref target="RFC5661" />, and similarly reads data from the
        regular file identified by the current filehandle.
      </t>

      <t>
        The client provides an offset of where the READPLUS is to start and a
        count of how many bytes are to be read.  An offset of zero means to
        read data starting at the beginning of the file.  If offset is
        greater than or equal to the size of the file, the status NFS4_OK is
        returned with nfs_readplusrestype4 set to READ_OK, data length set to
        zero, and eof set to TRUE.  The READPLUS is subject to access
        permissions checking.
      </t>

      <t>
        If the client specifies a count value of zero, the READPLUS succeeds
        and returns zero bytes of data, again subject to access permissions
        checking.  In all situations, the server may choose to return fewer
        bytes than specified by the client.  The client needs to check for
        this condition and handle the condition appropriately.
      </t>

      <t>
        If the client specifies an offset and count value that is entirely
        contained within a hole of the file, the status NFS4_OK is returned
        with nfs_readplusresok4 set to READ_HOLE, and if information is
        available regarding the hole, a nfs_readplusreshole structure
        containing the offset and range of the entire hole.  The
        nfs_readplusreshole structure is considered valid until the file is
        changed (detected via the change attribute).  The server MUST provide
        the same semantics for nfs_readplusreshole as if the client read the
        region and received zeroes; the implied holes contents lifetime MUST
        be exactly the same as any other read data.
      </t>

      <t>
        If the client specifies an offset and count value that begins in a
        non-hole of the file but extends into hole the server should return a
        short read with status NFS4_OK, nfs_readplusresok4 set to READ_OK,
        and data length set to the number of bytes returned.  The client will
        then issue another READPLUS for the remaining bytes, which the server
        will respond with information about the hole in the file.
      </t>

      <t>
        If the server knows that the requested byte range is into a hole of
        the file, but has no further information regarding the hole, it
        returns a nfs_readplusreshole structure with holeres4 set to
        HOLE_NOINFO.
      </t>

      <t>
        If hole information is available on the server and can be returned to
        the client, the server returns a nfs_readplusreshole structure with
        the value of holeres4 to HOLE_INFO.  The values of hole_offset and
        hole_length define the byte-range for the current hole in the file.
        These values represent the information known to the server and may
        describe a byte-range smaller than the true size of the hole.
      </t>

      <t>
        Except when special stateids are used, the stateid value for a
        READPLUS request represents a value returned from a previous byte-
        range lock or share reservation request or the stateid associated
        with a delegation.  The stateid identifies the associated owners if
        any and is used by the server to verify that the associated locks are
        still valid (e.g., have not been revoked).
      </t>

      <t>
        If the read ended at the end-of-file (formally, in a correctly formed
        READPLUS operation, if offset + count is equal to the size of the
        file), or the READPLUS operation extends beyond the size of the file
        (if offset + count is greater than the size of the file), eof is
        returned as TRUE; otherwise, it is FALSE.  A successful READPLUS of
        an empty file will always return eof as TRUE.
      </t>

      <t>
        If the current filehandle is not an ordinary file, an error will be
        returned to the client.  In the case that the current filehandle
        represents an object of type NF4DIR, NFS4ERR_ISDIR is returned.  If
        the current filehandle designates a symbolic link, NFS4ERR_SYMLINK is
        returned.  In all other cases, NFS4ERR_WRONG_TYPE is returned.
      </t>

      <t>
        For a READPLUS with a stateid value of all bits equal to zero, the
        server MAY allow the READPLUS to be serviced subject to mandatory
        byte-range locks or the current share deny modes for the file.  For a
        READPLUS with a stateid value of all bits equal to one, the server
        MAY allow READPLUS operations to bypass locking checks at the server.
      </t>

      <t>
        On success, the current filehandle retains its value.
      </t>
    </section>

    <section title="IMPLEMENTATION">
      <t>
        If the server returns a "short read" (i.e., fewer data than requested
        and eof is set to FALSE), the client should send another READPLUS to
        get the remaining data.  A server may return less data than requested
        under several circumstances.  The file may have been truncated by
        another client or perhaps on the server itself, changing the file
        size from what the requesting client believes to be the case.  This
        would reduce the actual amount of data available to the client.  It
        is possible that the server reduce the transfer size and so return a
        short read result.  Server resource exhaustion may also occur in a
        short read.
      </t>

      <t>
        If mandatory byte-range locking is in effect for the file, and if the
        byte-range corresponding to the data to be read from the file is
        WRITE_LT locked by an owner not associated with the stateid, the
        server will return the NFS4ERR_LOCKED error.  The client should try
        to get the appropriate READ_LT via the LOCK operation before re-
        attempting the READPLUS.  When the READPLUS completes, the client
        should release the byte-range lock via LOCKU.
      </t>

      <t>
        If another client has an OPEN_DELEGATE_WRITE delegation for the file
        being read, the delegation must be recalled, and the operation cannot
        proceed until that delegation is returned or revoked.  Except where
        this happens very quickly, one or more NFS4ERR_DELAY errors will be
        returned to requests made while the delegation remains outstanding.
        Normally, delegations will not be recalled as a result of a READPLUS
        operation since the recall will occur as a result of an earlier OPEN.
        However, since it is possible for a READPLUS to be done with a
        special stateid, the server needs to check for this case even though
        the client should have done an OPEN previously.
      </t>

      <section title="Additional pNFS Implementation Information">

        <t>
          With pNFS, the semantics of using READPLUS remains the same.  Any
          data server MAY return a READ_HOLE result for a READPLUS request that
          it receives.
        </t>

        <t>
          When a data server chooses to return a READ_HOLE result, it has a
          certain level of flexibility in how it fills out the
          nfs_readplusreshole structure.
        </t>

        <t>
          <list style="numbers">
            <t>
              For a data server that cannot determine any hole information, the
              data server SHOULD return HOLE_NOINFO.
            </t>

            <t>
              For a data server that can only obtain hole information for the
              parts of the file stored on that data server, the data server
              SHOULD return HOLE_INFO and the byte range of the hole stored on
              that data server.
            </t>

            <t>
              For a data server that can obtain hole information for the entire
              file without severe performance impact, it MAY return HOLE_INFO
               nd the byte range of the entire file hole.
            </t>
          </list>
        </t>

        <t>
          In general, a data server should do its best to return as much
          information about a hole as is feasible.  In general, pNFS server
          implementers should try ensure that data servers do not overload the
          metadata server with requests for information.  Therefore, if
          supplying global sparse information for a file to data servers can
          overwhelm a metadata server, then data servers should use option 1 or
          2 above.
        </t>

        <t>
          When a pNFS client receives a READ_HOLE result and a non-empty
          nfs_readplusreshole structure, it MAY use this information in
          conjunction with a valid layout for the file to determine the next
          data server for the next region of data that is not in a hole.
        </t>
      </section>
    </section>

    <section title="READPLUS with Sparse Files Example">

      <t>
        To see how the return value READ_HOLE will work, the following table
        describes a sparse file.  For each byte range, the file contains
        either non-zero data or a hole.
      </t>


      <texttable anchor="space_example">
        <ttcol align='left' >Byte-Range</ttcol>
        <ttcol align='left' >Contents</ttcol>

        <c>0-31999     </c> <c>  Non-Zero </c>
        <c>32K-255999  </c> <c>  Hole     </c>
        <c>256K-287999 </c> <c>  Non-Zero </c>
        <c>288K-353999 </c> <c>  Hole     </c>
        <c>354K-417999 </c> <c>  Non-Zero </c>
      </texttable>

      <t>
        Under the given circumstances, if a client was to read the file from
        beginning to end with a max read size of 64K, the following will be
        the result.  This assumes the client has already opened the file and
        acquired a valid stateid and just needs to issue READPLUS requests.
      </t>

      <t>
        <list style="numbers">
          <t>
            READPLUS(s, 0, 64K) --> NFS_OK, readplusrestype4 = READ_OK, eof =
            false, data&lt;&gt;[32K].  Return a short read, as the last half of the
             equest was all zeroes.
          </t>

          <t>
            READPLUS(s, 32K, 64K) --> NFS_OK, readplusrestype4 = READ_HOLE,
            nfs_readplusreshole(HOLE_INFO)(32K, 224K). The requested range was
            all zeros, and the current hole begins at offset 32K and is 224K
            in length.
          </t>

          <t>
            READPLUS(s, 256K, 64K) --> NFS_OK, readplusrestype4 = READ_OK, eof
            = false, data&lt;&gt;[32K].  Return a short read, as the last half of
            the request was all zeroes.
          </t>

          <t>
            READPLUS(s, 288K, 64K) --> NFS_OK, readplusrestype4 = READ_HOLE,
            nfs_readplusreshole(HOLE_INFO)(288K, 66K).
          </t>

          <t>
           READPLUS(s, 354K, 64K) --> NFS_OK, readplusrestype4 = READ_OK,
           eof = true, data&lt;&gt;[64K].
          </t>
        </list>
      </t>
    </section>
  </section>

  <section anchor="ss:sparse:related" title="Related Work">

    <t>
      Solaris and ZFS support an extension to lseek(2) that allows
      applications to discover holes in a file. The values, SEEK_HOLE and
      SEEK_DATA, allow clients to seek to the next hole or beginning of
      data, respectively.
    </t>

    <t>
      XFS supports the XFS_IOC_GETBMAP extended attribute, which returns
      the Data Region Map for a file. Clients can then use this information
      to avoid reading holes in a file.
    </t>

    <t>
      NTFS and CIFS support the FSCTL_SET_SPARSE attribute, which allows
      applications to control whether empty regions of the file are
      preallocated and filled in with zeros or simply left unallocated.
    </t>
  </section>

  <section anchor="ss:sparse:sec" title="Security Considerations">
    <t>
      The additions to the NFS protocol for supporting sparse file reads
      does not alter the security considerations of the NFSv4.1 protocol
      <xref target="RFC5661" />.
    </t>
  </section>

  <section anchor="ss:sparse:iana" title="IANA Considerations">
    <t>
      There are no IANA considerations in this section.
    </t>
  </section>
</section>
